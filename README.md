# Proyecto Final ğŸ’» - SegmentaciÃ³n de ImÃ¡genes de Leucocitos mediante SegFormer 
```
Universidad de Guanajuato - Campus LeÃ³n, DivisiÃ³n de Ciencias e IngenierÃ­as
Elaborado por: Aurora PinzÃ³n Arzola, a.pinzonarzola@ugto.mx
Programa educativo: Licenciatura en FÃ­sica
Curso: Temas Selectos de FÃ­sica (Aprendizaje AutomÃ¡tico)
Profesor: Dr. Luis Carlos Padierna GarcÃ­a
Ciclo escolar: Enero - Junio 2025
```
ğŸŸ¥ **IntroducciÃ³n**

ğŸ©¸ Los glÃ³bulos blancos o leucocitos forman parte del sistema inmune y participan en las respuestas inmunitarias innatas del cuerpo y en la producciÃ³n de anticuerpos. Circulan por la sangre y organizan respuestas inflamatorias y celulares ante lesiones o agentes patÃ³genos.

La visiÃ³n computacional es un campo de la Inteligencia Artificial basada en el aprendizaje profundo y las Redes Neuronales Convolucionales (CNN), ambas pertenecientes a Machine Learning, y cuyo uso ha aumentado en el campo de la salud debido a la gran cantidad de ventajas y buenos resultados reportados en la literatura.

En este proyecto se presenta la aplicaciÃ³n de visiÃ³n computacional atravÃ©s del uso de la segmentaciÃ³n semÃ¡ntica de imÃ¡genes de leucocitos mediante SegFormer, una poderosa arquitectura que unifica Transformers con decoders compuestos por perceptrÃ³n multicapa (MLP), con el propÃ³sito de contribuir a la automatizaciÃ³n del Conteo Diferencial de Leucocitos.

ğŸŸ§ **Objetivo**

* Implementar SegFormer para presentar un enfoque alternativo a la recreaciÃ³n de la tesis â€œESTUDIO E IMPLEMENTACIÃ“N DE TÃ‰CNICAS DE VISIÃ“N COMPUTACIONAL BASADAS EN APRENDIZAJE PROFUNDO PARA EL CONTEO DIFERENCIAL Y SEGMENTACIÃ“N DE LEUCOCITOS EN IMÃGENES DE ASPIRADO DE MÃ‰DULA Ã“SEAâ€, del licenciado Ãngel Daniel Canales RamÃ­rez.
* Presentar el proyecto y sus resultados utilizando un formato poco convencional, por ejemplo, a travÃ©s de un repositorio de GitHub, una pÃ¡gina Web, visiÃ³n aumentada, etc. 

ğŸŸ¨ **JustificaciÃ³n**

La implementaciÃ³n de tecnologÃ­as de visiÃ³n computacional en el Ã¡rea de la salud ha sido de gran relevancia al facilitar y agilizar la detecciÃ³n, clasificaciÃ³n y observaciÃ³n de elementos en imÃ¡genes mÃ©dicas. Es por ello por lo que, se considera importante poder estudiar y aprender estas tecnologÃ­as ya que han demostrado un potencial para automatizar y estandarizar el anÃ¡lisis de imÃ¡genes mÃ©dicas.

ğŸŸ© **Marco TeÃ³rico**

Los leucocitos son un tipo de cÃ©lula sanguÃ­nea producida en la mÃ©dula Ã³sea, y se encuentran en la sangre (representando aproximadamente 1% de nuestra sangre) y el tejido linfÃ¡tico (amÃ­gdalas, el timo, el bazo y la mÃ©dula Ã³sea). Los glÃ³bulos blancos forman parte del sistema inmune y participan en las respuestas inmunitarias innatas y producciÃ³n de anticuerpos.

El conteo diferencial de leucocitos es una prueba que se realiza para determinar la proporciÃ³n de diferentes tipos de leucocitos, mediante la clasificaciÃ³n de al menos 100 leucocitos consecutivos en neutrÃ³filos, neutrÃ³filos en banda, linfocitos, monocitos, eosinÃ³filos y basÃ³filos, mediante un frotis de sangre preparado y teÃ±ido. Para poder realizar este conteo es necesario, obtener una muestra de sangre, obtener un frotis y realizar la tinciÃ³n y preparaciÃ³n adecuada para llevar a cabo el conteo. 

Es por lo anterior que, con el propÃ³sito de contribuir a la automatizaciÃ³n del Conteo Diferencial de Leucocitos, se plantea implementar visiÃ³n computacional para facilitar y automatizar la identificaciÃ³n de leucocitos. 

Los Transformers se introdujeron en 2020 con el artÃ­culo â€œAttention Is All You Needâ€. Son un tipo de modelo de aprendizaje profundo que utilizan el mecanismo de atenciÃ³n para aumentar la velocidad de procesamiento y mejorar el rendimiento en tareas que implican datos secuenciales. Dos componentes importantes de esta arquitectura son: 
* Mecanismo de AtenciÃ³n: es posible procesar mÃºltiples partes de una secuencia de entrada simultÃ¡neamente al procesar cada parte individual, al calcular el attention score que captura la relaciÃ³n contextual entre cada palabra y el resto de la oraciÃ³n.
* 	Problema de Descenso de Gradiente (Backpropagation): problema comÃºn en redes neuronales, como RNN, en el que los gradientes se vuelven muy pequeÃ±os y los pesos de la red no se actualizan correctamente, llevando a un mal entrenamiento y rendimiento. 
 
Los Transfomers estÃ¡n constituÃ­dos por un encoder y un decoder. 

â¡ï¸ *ENCODER*:
Procesa la secuencia de entrada. Consiste de varias capas apiladas conformadas por subcapas: multi-head self-attention mechanism and the fully connected feed-forward network.
* Multi-head self-attention mechanism: procesa la secuencia de entrada al determinar la relevancia o â€œatenciÃ³nsâ€ que se le debe de asignar a diferentes partes del texto.
* Feed-forward network: aplica una transformaciÃ³n a la secuencia procesada por el mecanismo de atenciÃ³n.
  
â¡ï¸ *DECODER*:  
Genera la secuencia de salida basada en la informaciÃ³n del encoder, con la restricciÃ³n de Ãºnicamente utilizar salidas conocidas (masked self-attention). 

 **Segformer**

Para este proyecto, se utilizÃ³ el SegFormer, una arquitectura que unifica un codificador jerÃ¡rquico Transformer con un decodificador ligero de perceptrÃ³n multicapa (MLP). Su diseÃ±o destaca por dos caracterÃ­sticas principales:
*	Codificador Transformer jerÃ¡rquico: Genera caracterÃ­sticas multiescala a diferentes resoluciones, capturando tanto detalles finos como gruesos de la imagen. A diferencia de los transformadores tradicionales, SegFormer no utiliza codificaciones posicionales explÃ­citas, lo que lo hace resistente a las variaciones de resoluciÃ³n de imagen entre el entrenamiento y la inferencia.
*	Decodificador ligero basado en MLP: SegFormer utiliza un decodificador sencillo basado en MLP. Este decodificador fusiona caracterÃ­sticas de diferentes capas del codificador, combinando el contexto local y global para producir mÃ¡scaras de segmentaciÃ³n precisas de manera eficiente.

> ``ğŸ“`` **NOTA:** Mayor informaciÃ³n sobre el proyecto se puede encontrar en el video, la presentaciÃ³n y los cÃ³digos adjuntos en este repositorio. 

[![VIDEO:](https://img.youtube.com/vi/FNfDsF6lk5Q/maxresdefault.jpg)](https://youtu.be/FNfDsF6lk5Q)


